import torch
from torch import nn

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class CNN_36(nn.Module):
    def __init__(self, nChannel):
        super(CNN_36, self).__init__()

        self.layer0 = nn.Sequential(nn.Conv2d(in_channels=nChannel, out_channels=32, kernel_size=3, stride=1, padding=1),
                                    nn.ReLU(inplace=True),
                                    nn.MaxPool2d(kernel_size=2, stride=2))

        self.layer1 = nn.Sequential(nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),
                                    #nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),
                                    nn.ReLU(inplace=True),
                                    nn.MaxPool2d(kernel_size=2, stride=2))

        self.layer2 = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),
                                    nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),
                                    nn.ReLU(inplace=True),
                                    nn.MaxPool2d(kernel_size=(3, 1), stride=(3, 1)))

        self.layer3 = nn.Sequential(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(5, 5), stride=1, padding=0),
                                    nn.BatchNorm2d(256),
                                    nn.ReLU(inplace=True))
                                    #nn.AdaptiveMaxPool2d((1, 30)))

    def forward(self, input):
        output0 = self.layer0(input)
        output1 = self.layer1(output0)
        output2 = self.layer2(output1)
        output3 = self.layer3(output2)

        return output3
        
class myCRNN_0(nn.Module):
    def __init__(self, nChannel, output_size):
        super(myCRNN_0, self).__init__()
        self.cnn = nn.Sequential(CNN_36(nChannel))
        self.num_layers = 1
        self.n_directions = 2
        self.hidden_size = 64
        self.gru = nn.GRU(input_size=256, hidden_size=self.hidden_size,
                          num_layers=self.num_layers,
                          bidirectional=True, batch_first=True)
        self.fc = nn.Linear(self.hidden_size * self.n_directions, output_size)
        self.log_softmax = nn.LogSoftmax(2)


    def forward(self, input):
        x = self.cnn(input)  # [B,C,1,W]
        # pytorch框架输出结构为BCHW
        batch, channel, height, width = x.size()
        assert height == 1, "the output height must be 1."
        # 将height==1的维度去掉-->BCW
        x = x.squeeze(dim=2)
        # 调整各个维度的位置(B,C,W)->(W,B,C)，对应lstm的输入(seq,batch,input_size)
        x = x.permute(0, 2, 1)
        hidden = torch.zeros((self.num_layers * self.n_directions,
                              x.size(0), self.hidden_size), device=device)

        output, hidden = self.gru(x, hidden)
        x = self.fc(output)
        x = x.permute(1, 0, 2)
        output = self.log_softmax(x)
        output_lengths = torch.full(size=(x.size(1),), fill_value=x.size(0), dtype=torch.long, device=device)
        return output, output_lengths
